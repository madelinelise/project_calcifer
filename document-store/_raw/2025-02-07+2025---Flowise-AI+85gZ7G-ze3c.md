The OpenAI models that we use throughout
this series are incredibly powerful, but
they do come at a small cost.
And sometimes all you need is a chatbot
that runs on your local machine and that
won't break the bank.
In this video, I will show you how you
can build a RAG chatbot without spending
any money using Flowwise and Olama.
Olama is a tool that we can set up on our
local machines and allows
us to run LLM models locally.
It also provides integration with many
open source models like
LLM2, Mistral, Mixstral and more.
So in this video, I will show you how to
set up Olama and then we'll
download one of these models
and we will then integrate Olama in
Flowwise by first creating a simple
conversation chain chatbot
and lastly, we'll create a RAG chatbot
that will scrape
information from a website
and we'll be able to ask questions about
the content on that webpage.
Let's start by setting up Olama.
To get started, go to olama.com, then
click on download, then execute the file
that you just
downloaded and install Olama.
So after installation, that installation
pop-up will simply disappear,
but that's actually expected
and we will start Olama
manually using the terminal.
So on your machine, open the command
prompt, PowerShell or
terminal, whatever you prefer
and let's start by
downloading our first model.
The choice of model is extremely
important as some of these models are
extremely resource intensive.
So for this video, I'm actually going to
use the LLM2 model, but of course, I
recommend that you play with different
models based on your hardware.
In order to download a model, you can
simply click on the model name and on
this page, we can see
this command, Olama run LLM2.
Let's simply copy this command and let's
run this in the terminal.
The first time you run this, Olama will
first download the LLM2 model and you
will then see this
message saying, "send a message"
and if I enter anything like "hello", I
will get a response back from LLM2.
So there are actually a few ways to start
up Olama and in this example, we simply
started Olama using a
specific model like LLM2
and were able to chat with our model
using the terminal, which is
actually a lot of fun as well
and very useful for testing out the
responses from a given model.
But in order to make this model available
to Flow-wise, we'll
actually use a different approach.
So to cancel out of this,
I'm going to press Ctrl D.
Now let's serve Olama as a local server
by running "olama serve"
and by the way, if you ever receive this
error message, it simply means
that Olama is already running
and you can actually confirm this by
grabbing this URL and then
opening it in the browser
and you should see a message like this
that says "olama is running"
and Olama was actually
already running on my machine.
So I just quickly
cancelled the previous session.
So if we try this again by calling "olama
serve", this one I'll
start an Olama server
and listen for requests from Flow-wise.
So now that we have Olama installed and
running, let's create our
chat flows in Flow-wise.
Note that I am running a
local instance of Flow-wise.
So please ensure that you are also using
a local instance if
you are following along.
An instance running on render as example
will not work as it will not have access
to your local Olama instance.
This video is very specifically intended
for creating local
chatbots on your own machine.
Let's create a new chat flow.
Let's give it a name
like "local chatbot".
Let's save this and let's
start by adding a chain.
So under add nodes, go to chains and
let's add the conversation chain.
Let's add our chat model.
So under chat models,
let's add the chat Olama model.
Let's connect this model to our chain and
let's have a look at
this chat Olama node.
This takes in a URL as input and this is
the URL that our Olama
instance is being served on.
And if we copy this and open it in the
browser, you should see this
text saying "olama is running".
If you do not see this text, then please
ensure that you do have
your Olama server running.
If that is working, we then have to
specify our model name.
And the model that we
downloaded earlier was "Lama2".
You are able to download
multiple models using Olama.
So if you have more than one type of
model, you can easily swap
them out using this field.
Let's set the temperature to something
like 0.7 and let's
move on from that node.
Let's add our memory node.
So under add nodes, let's go to memory.
I'm going to add the buffer memory node
and let's connect that
to our chain as well.
And that is it.
Let's save this chat flow.
Let's test it out by
saying something like "hello".
And then less than a second, I got this
response back from Olama.
So we now have a completely free chat bot
that we can use on our local machines.
So let's go ahead and
create a rag chat bot.
This will allow us to upload a data
source like a website address, PDF
documents, text files,
or whatever else we want.
And we will then be able to ask the chat
bot questions about that data.
But before we continue, if you enjoy this
content, then please hit the like button,
subscribe to my channel, and please let
me know in the comments which
open source models you prefer.
Let's click on add.
Let's save this.
Let's call it "local rag chat bot".
And let's save this.
Let's start by adding a chain.
So under add nodes, let's go to chains.
And this time, let's add a
conversational retrieval QA chain.
This will allow us to add a
data source to this chain.
Let's add a chat model.
So under chat models,
let's go to chat Olama.
Let's connect our model to the chain.
Within the chat Olama node,
let's set the model as "Lama2".
And let's set the
temperature to a lower value like 0.4.
So let's add our data source by adding a
vector store retriever.
So under add nodes, let's
go down to vector stores.
And I'm just going to use the in-memory
vector store in this example.
But feel free to use pinecone, chroma, or
whatever you prefer.
Let's connect our
vector store to our chain.
And now we need to add
our embeddings model,
which is also slightly different to what
we're used to with openAI models.
Let's go to add nodes.
Then under embeddings, let's add the
Olama embedding node.
And let's connect
this to our vector store.
So for this node, we
can leave the base URL
the same as the base
URL for the chat model.
And for the model name, I
will simply use "Lama2" as well.
Then under additional parameters, I'm
going to enable "Use MMAP".
As I found that if I do not enable this,
the absurd actually fails.
This could be a bug, but at the moment,
that is what seems to work for me.
I'm going to close this popup, and let's
go ahead and add our data source.
Let's click on add nodes.
And under document loaders, you have the
option of uploading a wide variety of
different data sources.
But for this video, I'm going
to use the Cheerio web scraper
in order to scrape
information from a website.
Let's attach this document
loader to our vector store.
And under URL, I'm simply going to use
this link to the LangChai home page.
So I'm going to copy this, and
then paste it into this field.
And that's pretty much all we have to do
to set up our rag chatbot.
I'm going to save this chatbot.
Let's up-set this
information into the vector store.
And now that that's done, we should be
able to ask questions
about that web page.
So back in the chat, let's open this up,
and let's ask, what is LangSmith?
And indeed, we do get
the correct response back.
Using these open source models can
definitely be a lot of fun, but they do
have their limitations.
And as an example, it's very difficult to
create an agent with tools
using these open source models.
As function calling is
pretty much an open AI feature,
let me know in the comments if you would
like to see more videos
using these open source models.
